import glob
import os
import yaml

# Load the configuration file
configfile: "config.yaml"

# List all .fq and .fq.gz files in the data folder
input_files = glob.glob("data/*.fq") + glob.glob("data/*.fq.gz")

# Define the final output files
rule all:
    input:
        expand([
            "final_reads/{sample}_classified_homo_sapiens.fq", 
            "final_reads/{sample}_reads.lst",
            "final_reads/{sample}_classified_homo_sapiens_deaminated.bam", 
            "final_reads/{sample}_classified_homo_sapiens_non_deaminated.bam", 
            "final_report/{sample}.tsv"
        ], sample=[os.path.basename(f).split('.fq')[0] for f in input_files]),
        "final_report/combined_final_report.tsv"

# Compress all fastq files if not done before
rule gzip_fq_files:
    input:
        lambda wildcards: "data/" + wildcards.filename if not wildcards.filename.endswith('.gz') else "data/" + wildcards.filename[:-3]
    output:
        "data/{filename}.gz"
    benchmark:
        "benchmarks/gzip/{filename}.txt"
    shell:
        "gzip -c {input} > {output}"

# Map reads using bwa to the reference genome, convert to BAM, sort, and index
rule map_sort_index:
    input:
        "data/{sample}.fq.gz" 
    output:
        bam="mapping/{sample}.bam",
        bam_bai="mapping/{sample}.bam.bai",
        sai="temp/{sample}.sai",
        sam="temp/{sample}.sam"
    threads: config["threads"]
    params:
        ref_genome=config["ref_genome"]
    benchmark:
        "benchmarks/map_sort_index/{sample}.txt"
    shell:
        """
        bwa aln {params.ref_genome} -t {threads} -n 0.01 -o 2 -l 16500 {input} > {output.sai}
        bwa samse {params.ref_genome} {output.sai} {input} > {output.sam}
        samtools view -@ {threads} -h -b {output.sam} | samtools sort -@ {threads} -T temp/{wildcards.sample} -o {output.bam}
        samtools index {output.bam}
        """

if config["use_snp_panel"]:
    # Filter reads that overlap with SNP panel using bedtools
    rule on_target_read_filter:
        input:
            bam="mapping/{sample}.bam",
            bed=config["snp_panel_bed"]
        output:
            bam="mapping/{sample}_on_target.bam"
        benchmark:
            "benchmarks/on_target_read_filter/{sample}.txt"
        shell:
            "bedtools intersect -abam {input.bam} -b {input.bed} -u > {output.bam}"
else:
    rule on_target_read_filter:
        input:
            "mapping/{sample}.bam"
        output:
            "mapping/{sample}_on_target.bam"
        shell:
            "cp {input} {output}"

# Filter mapped reads with sequence length of at least 35 and mapping quality of at least 25
rule mapping_quality_filter:
    input:
        "mapping/{sample}_on_target.bam"
    output:
        "mapping/{sample}_L35MQ25.bam"
    benchmark:
        "benchmarks/mapping_quality_filter/{sample}.txt"
    shell:
        """
        samtools view -h -q 25 {input} |
        awk 'length($10) > 34 || $1 ~ /^@/' |
        samtools view -bS - > {output}
        """

# Mark duplicates
rule mark_duplicates:
    input:
        "mapping/{sample}_L35MQ25.bam"
    output:
        "mapping/{sample}_L35MQ25_dedup.bam"
    benchmark:
        "benchmarks/mark_duplicates/{sample}.txt"
    shell:
        """
        samtools sort -o mapping/{wildcards.sample}_L35MQ25_sorted.bam {input}
        samtools markdup -r mapping/{wildcards.sample}_L35MQ25_sorted.bam {output}
        rm mapping/{wildcards.sample}_L35MQ25_sorted.bam
        """

if config["mask_on_target_snps"]:
    # Mask on-target SNPs with 'N'
    rule mask_on_target_snps:
        input:
            "mapping/{sample}_L35MQ25_dedup.bam",
            snp_panel=config["snp_panel"]
        output:
            "mapping/{sample}_L35MQ25_dedup_masked.bam"
        benchmark:
            "benchmarks/mask_on_target_snps/{sample}.txt"
        shell:
            "python ../scripts/mask_on_target_snps_vernot_replicate.py {input[0]} {input[1]} {output}"
else:
    rule mask_on_target_snps:
        input:
            "mapping/{sample}_L35MQ25_dedup.bam"
        output:
            "mapping/{sample}_L35MQ25_dedup_masked.bam"
        shell:
            "cp {input} {output}"

# Convert BAM to fastq
rule bam_to_fastq:
    input:
        "mapping/{sample}_L35MQ25_dedup_masked.bam"
    output:
        "mapping/{sample}_L35MQ25_dedup_masked.fq"
    benchmark:
        "benchmarks/bam_to_fastq/{sample}.txt"
    shell:
        "samtools fastq {input} > {output}"

# Classification using Kraken2 or Centrifuge 
if (config['classification_software'] == "kraken2"): 
    rule classification:
        input:
            "mapping/{sample}_L35MQ25_dedup_masked.fq"
        output:
            kraken2_results="classification/{sample}_L35MQ25.kraken2",
            kraken2_report="classification/{sample}_L35MQ25.k2report"
        params:
            kraken2_path=config["classification_software_path"],
            kraken2_db=config["classification_index"]
        resources:
            mem_mb=config['memory_mb']  # in MB
        benchmark:
            "benchmarks/classification/{sample}.txt"
        shell:
            """
            {params.kraken2_path}/kraken2 --db {params.kraken2_path}/{params.kraken2_db} --output {output.kraken2_results} --report {output.kraken2_report} {input}
            """

    # Generate classified reads
    rule generate_classified_reads:
        input:
            kraken2_result="classification/{sample}_L35MQ25.kraken2",
            fq="mapping/{sample}_L35MQ25_dedup_masked.fq"
        output:
            fq="final_reads/{sample}_classified_homo_sapiens.fq",
            lst="final_reads/{sample}_reads.lst"
        params:
            taxID=config['taxID']  # Homo sapiens
        benchmark:
            "benchmarks/generate_classified_reads/{sample}.txt"
        shell:
            """
            python ../scripts/select_reads_from_kraken2.py {params.taxID} {input.kraken2_result} {wildcards.sample}
            mv {wildcards.sample}_reads.lst final_reads/
            /global/home/users/jieruixu/jieruixu/sediment_dna/seqtk/seqtk subseq {input.fq} {output.lst} > {output.fq}
            """
            
elif (config['classification_software'] == "centrifuge"): 
    # Classification
    rule classification:
        input:
            "mapping/{sample}_L35MQ25_dedup_masked.fq"
        output:
            "classification/{sample}_L35MQ25.centrifuge",
            "classification/{sample}_L35MQ25.centrifugeLog",
            "classification/{sample}_L35MQ25.k2report"
        params:
            centrifuge_path=config["classification_software_path"],
            centrifuge_db=config["classification_index"]
        threads: config['threads']
        resources:
            mem_mb=config['memory_mb']  # in MB
        benchmark:
            "benchmarks/classification/{sample}.txt"
        shell:
            """
            # Run Centrifuge classification
            {params.centrifuge_path}/centrifuge -x {params.centrifuge_path}/{params.centrifuge_db} -S {output[0]} --report-file {output[1]} -U {input} -p {threads}
            # Generate Centrifuge report
            {params.centrifuge_path}/centrifuge-kreport -x {params.centrifuge_path}/{params.centrifuge_db} {output[0]} > {output[2]}
            """

    # Generate classified reads
    rule generate_classified_reads:
        input:
            centrifuge_result="classification/{sample}_L35MQ25.centrifuge",
            fq="mapping/{sample}_L35MQ25_dedup_masked.fq"
        output:
            fq="final_reads/{sample}_classified_homo_sapiens.fq",
            lst="final_reads/{sample}_reads.lst"
        params:
            taxID=9606  # Homo sapiens
        benchmark:
            "benchmarks/generate_classified_reads/{sample}.txt"
        shell:
            """
            python ../scripts/select_reads_from_centrifuge.py {params.taxID} {input.centrifuge_result} {wildcards.sample}
            mv {wildcards.sample}_reads.lst final_reads/
            /global/home/users/jieruixu/jieruixu/sediment_dna/seqtk/seqtk subseq {input.fq} {output.lst} > {output.fq}
            """

# Map again, and run mapDamage
rule fastq_to_bam:
    input:
        fastq="final_reads/{sample}_classified_homo_sapiens.fq",
        bam="mapping/{sample}_L35MQ25_dedup_masked.bam"
    output:
        bam="mapping/{sample}_classified_homo_sapiens.bam",
        bam_bai="mapping/{sample}_classified_homo_sapiens.bam.bai",
        read_ids=temp("temp/{sample}_read_ids.txt")
    threads: config["threads"]
    benchmark:
        "benchmarks/map_after_classification/{sample}.txt"
    shell:
        """
        awk 'NR % 4 == 1 {{print substr($1, 2)}}' {input.fastq} > {output.read_ids}
        samtools view -h -@ {threads} -N {output.read_ids} {input.bam} | samtools sort -@ {threads} -o {output.bam}
        samtools index {output.bam}
        """

# if config['run_mapdamage']: 
#     # Run mapDamage after classification
#     rule run_mapdamage_after_classification:
#         input:
#             sam_file="mapping/{sample}_classified_homo_sapiens.bam",
#             ref_file=config["mapdamage_ref"]
#         output:
#             directory("mapdamage_result_after_classification/{sample}")
#         benchmark:
#             "benchmarks/run_mapdamage_after_classification/{sample}.txt"
#         shell:
#             """
#             mapDamage --merge-libraries -i {input.sam_file} -r {input.ref_file} -d {output} --no-stats
#             """

# Rule to filter deaminated reads
rule filter_deaminated_reads:
    input:
        bam_file="mapping/{sample}_classified_homo_sapiens.bam",
        ref_file=config["ref_genome"]
    output:
        deaminated_bam="final_reads/{sample}_classified_homo_sapiens_deaminated.bam",
        non_deaminated_bam="final_reads/{sample}_classified_homo_sapiens_non_deaminated.bam",
        deaminated_bai="final_reads/{sample}_classified_homo_sapiens_deaminated.bam.bai",
        non_deaminated_bai="final_reads/{sample}_classified_homo_sapiens_non_deaminated.bam.bai",
        report_file="temp/{sample}_ct_report.csv"
    benchmark:
        "benchmarks/filter_deaminated_reads/{sample}.txt"
    shell:
        """
        python ../scripts/filter_deaminated_reads.py {input.bam_file} {output.deaminated_bam} {output.non_deaminated_bam} {input.ref_file} {output.report_file}
        samtools index {output.deaminated_bam}
        samtools index {output.non_deaminated_bam}
        """

# Final report generation
rule final_report:
    input:
        pre_map="data/{sample}.fq.gz",
        post_map="mapping/{sample}.bam",
        post_map_on_target="mapping/{sample}_on_target.bam",
        post_map_qc="mapping/{sample}_L35MQ25.bam",
        post_map_qc_rmdup="mapping/{sample}_L35MQ25_dedup.bam",
        post_map_qc_classi="mapping/{sample}_classified_homo_sapiens.bam",
        deaminated_bam="final_reads/{sample}_classified_homo_sapiens_deaminated.bam",
        report_file="temp/{sample}_ct_report.csv"
    output:
        "final_report/{sample}.tsv"
    params:
        check_alternate_allele="../scripts/check_alternate_allele_expected_only.py",
        lineage_sites=config["lineage_sites"]
    shell:
        """
        id={wildcards.sample}
        
        # Total reads 
        total_reads=$(zcat {input.pre_map} | echo $((`wc -l`/4)))

        # Average read length for post-mapping BAM
        avg_read_length_pre_map=$(samtools view {input.post_map} | cut -f 10 | perl -ne 'chomp;print length($_) . "\n"' | awk '{{ sum += $1; n++ }} END {{ if (n > 0) print sum / n; else print "0" }}')
        
        # Mapped reads after bwa 
        mapped_reads=$(samtools view -c -F 4 {input.post_map})

        # On-target reads with SNP panel 
        on_target_reads=$(samtools view -c -F 4 {input.post_map_on_target})

        # Filtered reads after Q25 L30 
        total_reads_post_map_qc=$(samtools view -c {input.post_map_qc})
        
        # Unique reads
        unique_reads=$(samtools view -c -F 4 {input.post_map_qc_rmdup})
        
        # Average duplication rate 
        dup_rate=$(echo "scale=2; $total_reads_post_map_qc / $unique_reads" | bc)
        
        # Average read length for unique reads
        avg_read_length_post_map_qc=$(samtools view {input.post_map_qc_rmdup} | cut -f 10 | perl -ne 'chomp;print length($_) . "\n"' | awk '{{ sum += $1; n++ }} END {{ if (n > 0) print sum / n; else print "0" }}')

        # Classified Homo Sapiens 
        total_reads_classified=$(samtools view -c -F 4 {input.post_map_qc_classi})
        
        # Deaminated reads
        total_reads_deaminated=$(samtools view -c {input.deaminated_bam})
    
        # Proportion deaminated (among classified homo sapiens)
        proportion_deaminated=$(echo "scale=3; $total_reads_deaminated / $total_reads_classified" | bc) 
        
        # Proportion hominin derived  
        proportion_all=$(python {params.check_alternate_allele} -b {input.post_map_qc_classi} -v {params.lineage_sites} -t all)
        
        # Proportion hominin derived (deaminated fragments) 
        proportion_all_deaminated=$(python {params.check_alternate_allele} -b {input.deaminated_bam} -v {params.lineage_sites} -t all)
        
        # Proportion Neanderthal branch derived  
        proportion_neandertal=$(python {params.check_alternate_allele} -b {input.post_map_qc_classi} -v {params.lineage_sites} -t neandertal)

        # Proportion Neanderthal branch derived  (deaminated fragments) 
        proportion_neandertal_deaminated=$(python {params.check_alternate_allele} -b {input.deaminated_bam} -v {params.lineage_sites} -t neandertal)

        # Proportion Denisova branch derived 
        proportion_denisova=$(python {params.check_alternate_allele} -b {input.deaminated_bam} -v {params.lineage_sites} -t denisova)

        # Proportion Denisova branch derived (deaminated fragments) 
        proportion_denisova_deaminated=$(python {params.check_alternate_allele} -b {input.deaminated_bam} -v {params.lineage_sites} -t denisova)

        # Format C to T percentages at 5' and 3' ends as percentages with two decimal points
        ct_5=$(awk -F, 'NR==2 {{printf "%.2f%%", $1}}' {input.report_file})
        ct_3=$(awk -F, 'NR==2 {{printf "%.2f%%", $2}}' {input.report_file})
        
        # Output the final report
        echo -e "ID\tSequenced reads\taverage_read_length\tMapped reads\tOn-target reads\tFiltered reads (Q25L30)\tUnique reads\tAverage duplication rate\tHomo Sapiens filtered (with Centrifuge)\tproportion_deaminated\tproportion_hominin_derived\tproportion_hominin_derived_deaminated\tproportion_neandertal_derived\tproportion_neandertal_derived_deaminated\tproportion_denisova_derived\tproportion_denisova_derived_deaminated\tC to T percentage at 5' end\tC to T percentage at 3' end" > {output}

        echo -e "$id\t$total_reads\t$avg_read_length_pre_map\t$mapped_reads\t$on_target_reads\t$total_reads_post_map_qc\t$unique_reads\t$dup_rate\t$total_reads_classified\t$proportion_deaminated\t$proportion_all\t$proportion_all_deaminated\t$proportion_neandertal\t$proportion_neandertal_deaminated\t$proportion_denisova\t$proportion_denisova_deaminated\t$ct_5\t$ct_3" >> {output}
        """

# Combine all TSV files into one and delete the individual files
rule combine_reports:
    input:
        expand("final_report/{sample}.tsv", sample=[os.path.basename(f).split('.fq')[0] for f in input_files])
    output:
        "final_report/combined_final_report.tsv"
    shell:
        """
        (head -n 1 {input[0]} && tail -n +2 -q final_report/*.tsv) > {output}
        """